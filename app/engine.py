# engine.py â€”Â highâ€‘level indexing API
# ---------------------------------------------------------------------------
# ÐžÐ±Ð½Ð¾Ð²Ð»Ñ‘Ð½ 2025â€‘06â€‘04 | ChatGPTâ€‘o3 refactor
#   â€¢Â Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ðµ ASCIIâ€‘slug Ð¸Ð¼ÐµÐ½Ð° ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¹ (Chroma)
#   â€¢Â Ð‘Ð°Ð³â€‘Ñ„Ð¸ÐºÑ list_documents_for_folder (Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° exists)
#   â€¢Â ÐšÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ HuggingFaceEmbedding (x10 ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ)
#   â€¢Â Pathlib + logging Ð²Ð¼ÐµÑÑ‚Ð¾ print/strâ€‘path
#   â€¢Â Ð£Ð½Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð² Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ Ð¾Ð´Ð½Ð¾Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
# ---------------------------------------------------------------------------

from __future__ import annotations

import logging
import shutil
from functools import lru_cache
from pathlib import Path
from typing import Dict, List

import chromadb
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from slugify import slugify

from app.chunk_manager import semantic_chunking
from parsers.text_extraction import TextBlock, extract_blocks
from app.term_graph import extract_term_tags

# ---------------------------------------------------------------------------
# Paths & logger
# ---------------------------------------------------------------------------

DOCS_PATH = Path("data/docs")
INDEX_ROOT = Path("data/index")

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

FILE_SKIP_PREFIXES = ("~$", ".")
FILE_SKIP_SUFFIXES = (".tmp",)
FILE_SKIP_NAMES = {"thumbs.db", ".ds_store"}

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _safe_collection_name(name: str) -> str:
    """Convert arbitrary folder name â†’ ASCIIâ€‘slug valid for Chroma."""
    slug: str = slugify(name, lowercase=True, separator="_")[:63].strip("._-")
    return slug or "default"


@lru_cache(maxsize=1)
def _get_embed_model() -> HuggingFaceEmbedding:
    logger.info("Loading HuggingFace embedding model â€¦")
    return HuggingFaceEmbedding(model_name="paraphrase-multilingual-MiniLM-L12-v2")


# ---------------------------------------------------------------------------
# Public helpers â€”Â library info
# ---------------------------------------------------------------------------

def get_library_folders() -> List[str]:
    """Return list of topâ€‘level document folders inside data/docs."""
    if not DOCS_PATH.exists():
        return []
    return [p.name for p in DOCS_PATH.iterdir() if p.is_dir()]


def list_documents_for_folder(folder: str) -> List[str]:
    """Return list of *files* for the given folder (skip tmp/hidden)."""
    folder_path = DOCS_PATH / folder
    if not folder_path.exists():
        return []

    files: List[str] = []
    for p in folder_path.iterdir():
        if not p.is_file():
            continue
        if p.name.lower() in FILE_SKIP_NAMES:
            continue
        if p.name.startswith(FILE_SKIP_PREFIXES):
            continue
        if any(p.name.endswith(sfx) for sfx in FILE_SKIP_SUFFIXES):
            continue
        files.append(p.name)
    return files


def check_indexed_files(folder: str) -> Dict[str, bool]:
    """Map {filename: is_indexed}.

    Fast check: ÐµÑÐ»Ð¸ slugâ€‘Ð¿Ð°Ð¿ÐºÐ° ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ âŸ¶ ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð²ÑÐµ Ñ„Ð°Ð¹Ð»Ñ‹ ÑƒÐ¶Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹.
    Ð”Ð»Ñ Ð±Ð¾Ð»ÐµÐµ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ Ð¼Ð¾Ð¶Ð½Ð¾ Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ manifest.json, Ð½Ð¾ Ð¿Ð¾ÐºÐ° Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾.
    """
    folder_path = DOCS_PATH / folder
    if not folder_path.exists():
        return {}

    index_path = INDEX_ROOT / _safe_collection_name(folder)
    indexed = index_path.exists() and index_path.is_dir()
    return {f: indexed for f in list_documents_for_folder(folder)}

# ---------------------------------------------------------------------------
# Core indexing pipeline
# ---------------------------------------------------------------------------

def trigger_indexing(folder: str) -> None:
    """Extract â†’ chunk â†’ embed â†’ push to Chroma."""
    logger.info("[ðŸ“‚] Indexing folder: %s", folder)
    folder_path = DOCS_PATH / folder
    if not folder_path.exists():
        logger.warning("Folder does not exist: %s â€”Â skip", folder_path)
        return

    files = [folder_path / f for f in list_documents_for_folder(folder)]
    if not files:
        logger.warning("No eligible files inside %s â€”Â skip", folder)
        return

    # ---------------------------- Extraction & chunking --------------------
    all_chunks = []
    for fp in files:
        try:
            blocks = extract_blocks(fp)
            chunks = semantic_chunking(blocks, fp)
            all_chunks.extend(chunks)
        except Exception as exc:  # noqa: BLE001
            logger.exception("semantic_chunking failed for %s: %s", fp, exc)

    if not all_chunks:
        logger.info("[SKIP] No chunks produced â€” nothing to index")
        return

    logger.info("Total chunks: %d", len(all_chunks))

    # ---------------------------- Chroma save ------------------------------
    index_path = INDEX_ROOT / _safe_collection_name(folder)
    index_path.mkdir(parents=True, exist_ok=True)

    embed_model = _get_embed_model()
    Settings.embed_model = embed_model  #Â llama-index global
    Settings.llm = None                 # we don't need the LLM here

    client = chromadb.PersistentClient(path=str(index_path))
    collection = client.get_or_create_collection(
        _safe_collection_name(folder), metadata={"title": folder}
    )

    # wipe old docs (if any) â€” safer than dropping & recreating collection
    try:
        ids_to_drop = collection.get(include=[])["ids"] or []
        if ids_to_drop:
            collection.delete(ids=ids_to_drop)
    except Exception as exc:  # noqa: BLE001
        logger.warning("Could not wipe collection: %s", exc)

    documents = [ch["text"] for ch in all_chunks]
    metadatas = [
        {
            "file_path": str(ch["file"]),
            "page_label": ch.get("page"),
            "type": ch.get("type", "text"),
            "chunk_level": ch.get("chunk_level", "atomic"),
            "parent_id": ch.get("parent_id"),
            "section_id": ch.get("section_id"),
            "term_tags": extract_term_tags(ch.get("text", "")),
        }
        for ch in all_chunks
    ]
    ids = [str(ch["id"]) for ch in all_chunks]

    collection.add(documents=documents, metadatas=metadatas, ids=ids)
    logger.info("[OK] Index saved to %s", index_path)


# ---------------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------------

def rebuild_index_from_folders(folders: List[str]) -> None:
    for f in folders:
        trigger_indexing(f)


def delete_index(folder: str) -> None:
    index_path = INDEX_ROOT / _safe_collection_name(folder)
    if index_path.exists():
        shutil.rmtree(index_path)
        logger.info("[ðŸ—‘] Index folder removed: %s", index_path)


def get_file_text(folder: str, file: str) -> str:
    """Return plain text of a single source document (no embeddings)."""
    file_path = DOCS_PATH / folder / file
    try:
        blocks: list[TextBlock] = extract_blocks(file_path)
        text_blocks = [b["text"] for b in blocks if b.get("type", "text") == "text"]
        return "\n\n".join(part for part in text_blocks if part.strip())
    except Exception as exc:  # noqa: BLE001
        logger.error("get_file_text failed for %s: %s", file_path, exc)
        return ""
