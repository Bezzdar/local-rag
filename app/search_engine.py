"""search_engine.py ‚Äî¬†LLM‚Äëdriven —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º
-----------------------------------------------------------------
–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–∞—è —á–∏—Å—Ç–∫–∞ (v0.1¬†/¬†2025‚Äë06‚Äë04)
¬†¬†‚Ä¢¬†–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ + type¬†hints
¬†¬†‚Ä¢¬†–†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ LLM
¬†¬†‚Ä¢¬†Pre‚Äënormalize —Å–ª–æ–≤–∞—Ä–∏ AND/OR/NOT
¬†¬†‚Ä¢¬†–ù–∏–∫–∞–∫–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Å–ª–æ–º–∞–Ω–∞ ‚Äî API –ø—Ä–µ–∂–Ω–∏–π
"""
from __future__ import annotations

import json
import logging
import re
from typing import Dict, List, Sequence, Tuple

from app.llm_generic import ask_llm
from app.user_settings import get_analytical_server_url

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# üîé –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ---------------------------------------------------------------------------
_JSON_RE = re.compile(r"\{.*\}", re.DOTALL)

_DEFAULT_QUERY: Dict[str, List[str]] = {"AND": [], "OR": [], "NOT": []}


def _extract_json(text: str) -> Dict[str, List[str]]:
    """–ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –≤—ã—É–¥–∏—Ç—å JSON‚Äë–æ–±—ä–µ–∫—Ç –≤–∏–¥–∞ {"AND": [...], ...} –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

    –ï—Å–ª–∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –Ω–µ —É–¥–∞—ë—Ç—Å—è ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å _DEFAULT_QUERY.
    """
    match = _JSON_RE.search(text)
    if not match:
        logger.debug("LLM returned no JSON: %s", text[:120])
        return _DEFAULT_QUERY.copy()

    try:
        raw = json.loads(match.group(0))
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –µ—Å—Ç—å –≤—Å–µ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è‚Äë—Å–ø–∏—Å–∫–∏ —Å—Ç—Ä–æ–∫
        parsed = {k.upper(): [str(w) for w in v] for k, v in raw.items() if k.upper() in _DEFAULT_QUERY}
        return {**_DEFAULT_QUERY, **parsed}
    except Exception as exc:  # pylint: disable=broad-except
        logger.debug("JSON parse failed: %s", exc, exc_info=False)
        return _DEFAULT_QUERY.copy()


# ---------------------------------------------------------------------------
# üß† LLM‚Äë–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
# ---------------------------------------------------------------------------

def llm_generate_query(
    user_prompt: str,
    last_chunks: Sequence[dict] | None = None,
    prev_queries: Sequence[dict] | None = None,
) -> Tuple[Dict[str, List[str]], str]:
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—É—é —Ñ–æ—Ä–º—É–ª—É AND/OR/NOT —Å –ø–æ–º–æ—â—å—é LLM.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂: (query_dict, raw_response).
    """
    context_fragments = " ".join(ch["text"][:100] for ch in last_chunks) if last_chunks else ""
    history = str(prev_queries) if prev_queries else ""

    prompt = (
        "–ù–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–æ–∏—Å–∫–æ–≤—É—é —Ñ–æ—Ä–º—É–ª—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n"
        f"–ó–∞–ø—Ä–æ—Å: {user_prompt}\n"
        + (f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã: {context_fragments}\n" if context_fragments else "")
        + (f"–ò—Å—Ç–æ—Ä–∏—è —É—Ç–æ—á–Ω–µ–Ω–∏–π: {history}\n" if history else "")
        + "–í–µ—Ä–Ω–∏ JSON —Å –∫–ª—é—á–∞–º–∏ AND, OR, NOT.\n"
        + "–ü—Ä–∏–º–µ—Ä:\n{\"AND\": [\"–∫–æ—Ä—Ä–æ–∑–∏—è\", \"—Ä–µ–∑–µ—Ä–≤—É–∞—Ä\"], \"NOT\": [\"–æ—á–∏—Å—Ç–∫–∞\"], \"OR\": [\"—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ\", \"–¥–µ—Ñ–µ–∫—Ç\"]}"
    )

    raw = ask_llm(prompt, server_url=get_analytical_server_url())
    query = _extract_json(raw)
    return query, raw


# ---------------------------------------------------------------------------
# ‚ö° –ë—ã—Å—Ç—Ä—ã–π –±—É–ª–µ–≤ –ø–æ–∏—Å–∫ –ø–æ —Å–ø–∏—Å–∫—É —á–∞–Ω–∫–æ–≤
# ---------------------------------------------------------------------------

def run_fast_search(
    query: Dict[str, Sequence[str]],
    all_chunks: Sequence[dict],
    top_n: int = 30,
) -> List[dict]:
    """–ü—Ä—è–º–æ–ª–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ AND/OR/NOT‚Äë—Å–ª–æ–≤–∞–º –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.¬†O(N)."""

    # –ü—Ä–µ–¥‚Äë–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è speed‚Äëup
    and_words = [w.lower() for w in query.get("AND", [])]
    or_words = [w.lower() for w in query.get("OR", [])]
    not_words = [w.lower() for w in query.get("NOT", [])]

    found: List[dict] = []
    for chunk in all_chunks:
        text = chunk.get("text", "").lower()
        if (
            all(word in text for word in and_words)
            and not any(word in text for word in not_words)
            and (not or_words or any(word in text for word in or_words))
        ):
            found.append(chunk)
            if len(found) >= top_n:
                break  # üí® —Ä–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥
    return found


# ---------------------------------------------------------------------------
# üìÉ LLM‚Äë—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
# ---------------------------------------------------------------------------

def llm_summarize_chunks(chunks: Sequence[dict], user_prompt: str) -> str:
    """–ü–æ–ø—Ä–æ—Å–∏—Ç—å LLM –≤—ã–¥–µ–ª–∏—Ç—å –Ω—É–∂–Ω–æ–µ –∏–∑ —Å–ø–∏—Å–∫–∞ —á–∞–Ω–∫–æ–≤."""
    if not chunks:
        return "–ù–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤."

    # –û–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –Ω–µ –∑–∞–ª–∏—Ç—å LLM –æ–≥—Ä–æ–º–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º (>4k —Ç–æ–∫–µ–Ω–æ–≤)
    context_items = []
    tokens = 0
    for ch in chunks:
        snippet = ch["text"][:400]
        tokens += len(snippet) // 4  # ~–≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞
        if tokens > 3500:
            break
        context_items.append(f"- {snippet}")

    context = "\n\n".join(context_items)
    prompt = (
        "–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–æ–ª—å–∫–æ —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤:\n" f"{context}\n" f"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {user_prompt}\n"
    )

    return ask_llm(prompt, server_url=get_analytical_server_url())
