"""search_engine.py ‚Äî¬†LLM‚Äëdriven —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º
-----------------------------------------------------------------
–ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–∞—è —á–∏—Å—Ç–∫–∞ (v0.1¬†/¬†2025‚Äë06‚Äë04)
¬†¬†‚Ä¢¬†–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ + type¬†hints
¬†¬†‚Ä¢¬†–†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ LLM
¬†¬†‚Ä¢¬†Pre‚Äënormalize —Å–ª–æ–≤–∞—Ä–∏ AND/OR/NOT
¬†¬†‚Ä¢¬†–ù–∏–∫–∞–∫–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Å–ª–æ–º–∞–Ω–∞ ‚Äî API –ø—Ä–µ–∂–Ω–∏–π
"""
from __future__ import annotations

import json
import logging
import re
from typing import Dict, List, Sequence, Tuple

from app.llm_generic import ask_llm
from app.user_settings import get_analytical_server_url
from app.term_graph import expand_terms_with_graph

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# üîé –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ---------------------------------------------------------------------------
_JSON_RE = re.compile(r"\{.*\}", re.DOTALL)

_DEFAULT_QUERY: Dict[str, List[str]] = {"AND": [], "OR": [], "NOT": []}

# –ë–∞–∑–æ–≤–∞—è –¥–æ–º–µ–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤/–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –¥–ª—è —Ç–µ—Ö–¥–æ–∫–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å).
_TERM_SYNONYMS: Dict[str, Sequence[str]] = {
    "–∫–∏–ø–∏–∞": ("–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ-–∏–∑–º–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–±–æ—Ä—ã", "–∞–≤—Ç–æ–º–∞—Ç–∏–∫–∞", "–ø—Ä–∏–±–æ—Ä—ã"),
    "—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥": ("—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥—ã", "–º–∞–≥–∏—Å—Ç—Ä–∞–ª—å", "–ª–∏–Ω–∏—è"),
    "–∫–æ—Ä—Ä–æ–∑–∏—è": ("–∫–æ—Ä—Ä–æ–∑–∏–æ–Ω–Ω—ã–π", "—Ä–∂–∞–≤—á–∏–Ω–∞", "–æ–∫–∏—Å–ª–µ–Ω–∏–µ"),
    "–¥–µ—Ñ–µ–∫—Ç": ("–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ", "—Ç—Ä–µ—â–∏–Ω–∞", "—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ"),
    "—Ä–µ–∑–µ—Ä–≤—É–∞—Ä": ("–µ–º–∫–æ—Å—Ç—å", "–±–∞–∫", "—Ç–∞–Ω–∫"),
    "–¥–∞–≤–ª–µ–Ω–∏–µ": ("p", "–∏–∑–±—ã—Ç–æ—á–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ"),
    "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞": ("t", "–Ω–∞–≥—Ä–µ–≤", "–æ—Ö–ª–∞–∂–¥–µ–Ω–∏–µ"),
}


def _norm_term(term: str) -> str:
    return term.strip().lower().replace("—ë", "–µ")


def _expand_term_variants(term: str) -> set[str]:
    normalized = _norm_term(term)
    if not normalized:
        return set()

    variants = {normalized}
    # forward map
    variants.update(_norm_term(v) for v in _TERM_SYNONYMS.get(normalized, ()))
    # reverse map
    for key, syns in _TERM_SYNONYMS.items():
        if normalized == _norm_term(key) or normalized in {_norm_term(v) for v in syns}:
            variants.add(_norm_term(key))
            variants.update(_norm_term(v) for v in syns)

    # graph expansion (Variant 3): related process/equipment/measurement terms
    variants.update(_norm_term(v) for v in expand_terms_with_graph(variants, depth=1, max_terms=20))
    return {v for v in variants if v}


def _expand_query_groups(words: Sequence[str]) -> list[set[str]]:
    groups: list[set[str]] = []
    for word in words:
        variants = _expand_term_variants(word)
        if variants:
            groups.append(variants)
    return groups


def _group_match(text: str, groups: Sequence[set[str]], mode: str) -> bool:
    if not groups:
        return True if mode == "all" else False

    checks = [any(variant in text for variant in variants) for variants in groups]
    return all(checks) if mode == "all" else any(checks)


def _extract_json(text: str) -> Dict[str, List[str]]:
    """–ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –≤—ã—É–¥–∏—Ç—å JSON‚Äë–æ–±—ä–µ–∫—Ç –≤–∏–¥–∞ {"AND": [...], ...} –∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

    –ï—Å–ª–∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –Ω–µ —É–¥–∞—ë—Ç—Å—è ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å _DEFAULT_QUERY.
    """
    match = _JSON_RE.search(text)
    if not match:
        logger.debug("LLM returned no JSON: %s", text[:120])
        return _DEFAULT_QUERY.copy()

    try:
        raw = json.loads(match.group(0))
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –µ—Å—Ç—å –≤—Å–µ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è‚Äë—Å–ø–∏—Å–∫–∏ —Å—Ç—Ä–æ–∫
        parsed = {k.upper(): [str(w) for w in v] for k, v in raw.items() if k.upper() in _DEFAULT_QUERY}
        return {**_DEFAULT_QUERY, **parsed}
    except Exception as exc:  # pylint: disable=broad-except
        logger.debug("JSON parse failed: %s", exc, exc_info=False)
        return _DEFAULT_QUERY.copy()


# ---------------------------------------------------------------------------
# üß† LLM‚Äë–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
# ---------------------------------------------------------------------------

def llm_generate_query(
    user_prompt: str,
    last_chunks: Sequence[dict] | None = None,
    prev_queries: Sequence[dict] | None = None,
) -> Tuple[Dict[str, List[str]], str]:
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—É—é —Ñ–æ—Ä–º—É–ª—É AND/OR/NOT —Å –ø–æ–º–æ—â—å—é LLM.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂: (query_dict, raw_response).
    """
    context_fragments = " ".join(ch["text"][:100] for ch in last_chunks) if last_chunks else ""
    history = str(prev_queries) if prev_queries else ""

    prompt = (
        "–ù–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–æ–∏—Å–∫–æ–≤—É—é —Ñ–æ—Ä–º—É–ª—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n"
        f"–ó–∞–ø—Ä–æ—Å: {user_prompt}\n"
        + (f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã: {context_fragments}\n" if context_fragments else "")
        + (f"–ò—Å—Ç–æ—Ä–∏—è —É—Ç–æ—á–Ω–µ–Ω–∏–π: {history}\n" if history else "")
        + "–í–µ—Ä–Ω–∏ JSON —Å –∫–ª—é—á–∞–º–∏ AND, OR, NOT.\n"
        + "–ü—Ä–∏–º–µ—Ä:\n{\"AND\": [\"–∫–æ—Ä—Ä–æ–∑–∏—è\", \"—Ä–µ–∑–µ—Ä–≤—É–∞—Ä\"], \"NOT\": [\"–æ—á–∏—Å—Ç–∫–∞\"], \"OR\": [\"—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ\", \"–¥–µ—Ñ–µ–∫—Ç\"]}"
    )

    raw = ask_llm(prompt, server_url=get_analytical_server_url())
    query = _extract_json(raw)
    return query, raw


# ---------------------------------------------------------------------------
# ‚ö° –ë—ã—Å—Ç—Ä—ã–π –±—É–ª–µ–≤ –ø–æ–∏—Å–∫ –ø–æ —Å–ø–∏—Å–∫—É —á–∞–Ω–∫–æ–≤
# ---------------------------------------------------------------------------

def run_fast_search(
    query: Dict[str, Sequence[str]],
    all_chunks: Sequence[dict],
    top_n: int = 30,
) -> List[dict]:
    """–ü—Ä—è–º–æ–ª–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ AND/OR/NOT‚Äë—Å–ª–æ–≤–∞–º –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.¬†O(N)."""

    # –ü—Ä–µ–¥‚Äë–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏/–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏
    and_groups = _expand_query_groups([str(w) for w in query.get("AND", [])])
    or_groups = _expand_query_groups([str(w) for w in query.get("OR", [])])
    not_groups = _expand_query_groups([str(w) for w in query.get("NOT", [])])

    ranked: list[tuple[int, dict]] = []
    for chunk in all_chunks:
        text = _norm_term(chunk.get("text", ""))

        and_ok = _group_match(text, and_groups, mode="all")
        not_ok = not _group_match(text, not_groups, mode="any") if not_groups else True
        or_ok = True if not or_groups else _group_match(text, or_groups, mode="any")

        if and_ok and not_ok and or_ok:
            score = 0
            score += sum(any(v in text for v in grp) for grp in and_groups) * 4
            score += sum(any(v in text for v in grp) for grp in or_groups) * 2
            # boost by sheer graph-term coverage inside chunk
            coverage = sum(text.count(v) for grp in and_groups + or_groups for v in grp)
            score += min(coverage, 20)
            ranked.append((score, chunk))

    ranked.sort(key=lambda x: x[0], reverse=True)
    return [ch for _, ch in ranked[:top_n]]


# ---------------------------------------------------------------------------
# üìÉ LLM‚Äë—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
# ---------------------------------------------------------------------------

def llm_summarize_chunks(chunks: Sequence[dict], user_prompt: str) -> str:
    """–ü–æ–ø—Ä–æ—Å–∏—Ç—å LLM –≤—ã–¥–µ–ª–∏—Ç—å –Ω—É–∂–Ω–æ–µ –∏–∑ —Å–ø–∏—Å–∫–∞ —á–∞–Ω–∫–æ–≤."""
    if not chunks:
        return "–ù–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤."

    # –û–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –Ω–µ –∑–∞–ª–∏—Ç—å LLM –æ–≥—Ä–æ–º–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º (>4k —Ç–æ–∫–µ–Ω–æ–≤)
    context_items = []
    tokens = 0
    for ch in chunks:
        snippet = ch["text"][:400]
        tokens += len(snippet) // 4  # ~–≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞
        if tokens > 3500:
            break
        context_items.append(f"- {snippet}")

    context = "\n\n".join(context_items)
    prompt = (
        "–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–æ–ª—å–∫–æ —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤:\n" f"{context}\n" f"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {user_prompt}\n"
    )

    return ask_llm(prompt, server_url=get_analytical_server_url())
