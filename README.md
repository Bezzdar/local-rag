# RAG Assistant (локальный проект)

Тестовый код для RAG‑ассистента с локальной индексацией документов, извлечением текста из файлов, чанкизацией и поиском по ChromaDB.

---

## 1) Общее назначение

Проект собирает документы из `data/docs/`, извлекает из них текст, разбивает на семантические чанки, индексирует в локальную векторную БД и использует эти данные в поиске/ответах ассистента.

Ключевые сценарии:
- индексация папки документов;
- редактирование/просмотр чанков;
- гибридный поиск (sparse + dense, в зависимости от настроек);
- запуск через Streamlit UI.

---

## 2) Подробная структура репозитория

### Корень проекта

- `streamlit_app.py` — основной интерфейс приложения (Streamlit), точка входа для пользовательской работы с RAG.
- `requirements.txt` — Python-зависимости проекта.
- `README.md` — основная документация по структуре и текущей реализации.
- `setup_venv.bat`, `start_app.bat` — вспомогательные скрипты запуска под Windows.

### Папка `app/` (основная бизнес-логика)

- `app/engine.py` — high-level API индексации:
  - выбор документов;
  - прогон extraction + chunking;
  - сохранение в Chroma;
  - утилиты удаления/перестроения индекса.
- `app/chunk_manager.py` — менеджер чанков и локальных хранилищ:
  - построение parent/child чанков;
  - фильтрация/поиск по чанкам;
  - операции загрузки/сохранения и вспомогательные CRUD-функции.
- `app/search_engine.py` — логика retrieval и поиска по индексу.
- `app/search_tools.py` — вспомогательные search-инструменты (в т.ч. TF‑IDF функции).
- `app/async_search.py` — асинхронные варианты поисковых операций.
- `app/term_graph.py` — извлечение терминов и граф терминов для обогащения retrieval.
- `app/llm_generic.py` — универсальный слой вызовов LLM.
- `app/math_engine.py` — вычислительные/математические вспомогательные механизмы.
- `app/chunk_editor.py` — отдельный UI/утилиты для редактирования чанков.
- `app/config.py` — конфигурация путей и runtime-параметров.
- `app/user_settings.py`, `app/user_settings.json` — пользовательские настройки и их хранение.

### Папка `parsers/` (извлечение текста и NLP-предобработка)

- `parsers/text_extraction.py` — центральный модуль парсинга документов:
  - поддержка форматов: PDF, DOCX/DOC, TXT/LOG;
  - очистка текста;
  - удаление повторяющихся межстраничных шумов в PDF (колонтитулы/штампы);
  - section-aware extraction (разбиение на разделы);
  - единый контракт блоков `TextBlock`;
  - опциональная TextRank-сегментация (с guardrail на длинных текстах);
  - семантическая чанкизация (`semantic_chunk`) с overlap и дедупликацией.
- `parsers/preprocessing.py` — сегментация и лемматизация (Natasha), lazy-init моделей.
- `parsers/ner_extraction.py` — извлечение именованных сущностей (Natasha NamesExtractor), lazy-init.

### Папка `data/`

- `data/docs/` — входные документы для индексации (библиотека источников).
- `data/index/` — persisted индексы Chroma по папкам.
- `data/chunks/` — локальные JSON-артефакты с чанками (когда сохранение включено).

---

## 3) Текущий контракт данных парсера (важно)

`extract_blocks(...)` из `parsers/text_extraction.py` возвращает **`list[TextBlock]`**.

Структура `TextBlock`:
- `text`: текст блока;
- `type`: тип блока (обычно `text`);
- `page`: номер страницы/маркер страницы;
- `source`: путь к исходному файлу;
- `section_id`: стабильный идентификатор раздела (например `p1.s2`);
- `section_title`: заголовок раздела.

Зачем это нужно:
- единый и предсказуемый формат между parser → chunk_manager → engine;
- возможность обогащать retrieval метаданными разделов;
- меньше интеграционных ошибок при развитии pipeline.

---

## 4) Как работает pipeline индексации

1. Пользователь выбирает папку в `data/docs/...`.
2. `app/engine.py` получает список файлов папки.
3. Для каждого файла вызывается `parsers.text_extraction.extract_blocks(...)`.
4. `app/chunk_manager.semantic_chunking(...)` строит parent/child чанки с метаданными секции.
5. Формируются документы и метаданные.
6. Данные пишутся в Chroma (`data/index/...`).

---

## 5) Что уже оптимизировано в pipeline для качества и скорости RAG

Реализовано:
- удаление повторяющихся межстраничных шумовых строк в PDF;
- section-aware extraction (перед чанкизацией);
- унифицированный `TextBlock` контракт;
- guardrails для TextRank на длинных документах;
- адаптивный размер чанков + overlap;
- дедупликация чанков на базе Simhash;
- graceful degradation при отсутствии части NLP-зависимостей (`razdel` / `sumy`);
- предрасчёт нормализованных словарей синонимов (forward/reverse) для быстрого boolean-search;
- кэш расширения терминов (`lru_cache`) + short-circuit проверка групп (`all/any`) без промежуточных списков;
- оптимизация top‑N в быстром поиске через `heapq.nlargest` при большом числе кандидатов;
- кэширование TF‑IDF/эмбеддингов по детерминированному fingerprint корпуса и ускоренный top‑k (`argpartition`).

Ожидаемый эффект:
- меньше шумовых совпадений при retrieval;
- лучшее удержание контекста на границах чанков;
- более стабильная релевантность top-k выдачи.

---

## 6) Рекомендованные следующие улучшения

Высокий приоритет:
- нормализация таблиц/списков под retrieval (табличные summary + markdown-форма);
- доменные шаблоны очистки техдоков (регистрационные хвосты, версии, инвентарные маркеры);
- метрики качества чанков (recall@k, доля коротких/шумовых чанков, A/B стратегий).

Средний приоритет:
- подключение лемматизации (`preprocessing.py`) как опционального режима для sparse-search;
- обогащение chunk metadata через NER (`ner_extraction.py`) для фильтров/reranking.

---

## 7) Быстрый старт

### Вариант для Windows через bat-файлы

1. Выполнить `setup_venv.bat` (создаст `./venv` и установит все зависимости).
2. Выполнить `start_app.bat` (запустит Streamlit и откроет приложение в браузере).

### Ручной вариант

1. Установить зависимости:
   - `pip install -r requirements.txt`
2. Запустить приложение:
   - `streamlit run streamlit_app.py`
3. Положить документы в `data/docs/<ваша_папка>/`.
4. Запустить индексацию через UI (кнопка индексации в Streamlit-приложении).

---

## 8) Примечания по совместимости

- Часть окружений может не иметь всех NLP-зависимостей; парсер поддерживает fallback-режимы.
- Для `.doc` (не `.docx`) в общем случае может потребоваться внешний конвертер в зависимости от качества исходных файлов.
